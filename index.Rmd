---
title: "Computational Musicology Portfolio"
name: "Sven"
output: 
  flexdashboard::flex_dashboard
---

```{r load packages, include=FALSE}
library(flexdashboard)
library(plotly)
library(ggplot2)
library(tidyverse)
library(readr)
library(DT)
library(rjson)
library(ggdendro)
library(heatmaply)
library(tidymodels)
library(kknn)
library(randomForest)
library(ranger)
```

```{r load source}
source("compmus 17.26.21.R")

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  
```

``` {r load data frame, include=FALSE}
compmus2025 <- read.csv2("compmus2025.csv", sep = ";")
```

 ```{r create danceability table, include=FALSE}
# compmus2025_dance <- 
#  compmus2025 %>%
#    group_by(Names) %>%
#    summarize(meanDanceability = mean(danceability))
 ```


Clustering and Heatmap {data-navmenu="Clustering and Classification"}
==============================================================================

```{r pre-processing}
cluster_juice <-
  recipe(
    filename ~
      arousal +
      danceability +
      instrumentalness +
      tempo +
      valence,
    data = compmus2025
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  prep(compmus2025) |>
  juice() |>
  column_to_rownames("filename")
```

```{r computing distances}
compmus_dist <- dist(cluster_juice, method = "euclidean")
```


Clustering and Heatmap {.tabset}
-------------------------------------------------------------------
### Hierarchical clustering

```{r hierarchical clustering}
compmus_dist |> 
  hclust(method = "single") |> 
  dendro_data() |>
  ggdendrogram()
```

> I still need to filter out a few examples I want to use for this, but I do not really know how... Should I filter on  few names or a certain characteristic?

### Heatmap
```{r heatmap}
heatmaply(
  cluster_juice,
  hclustfun = hclust,
  hclust_method = "average",
  dist_method = "euclidean"
)
```

> I still need to filter out a few examples I want to use for this, but I do not really know how... Should I filter on  few names or a certain characteristic?

Classification {data-navmenu="Clustering and Classification"}
=========================================================================================
Graphs
-------------------------------------------------------------------------------
```{r filtering for AI}
compmus2025_filtered <- 
  compmus2025 |> filter(!is.na(ai)) |> 
  mutate(ai = factor(if_else(ai, "AI", "Non-AI")))
```

```{r pre-processing 2}
classification_recipe <-
  recipe(
    ai ~
      arousal +
      danceability +
      instrumentalness +
      tempo +
      valence,
    data = compmus2025_filtered
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())
```

```{r cross-validation}
compmus_cv <- compmus2025_filtered |> vfold_cv(5)
```

```{r k nearest neighboour}
knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")
classification_knn <- 
  workflow() |> 
  add_recipe(classification_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(compmus_cv, control = control_resamples(save_pred = TRUE))
```

### Mosaic matrix
```{r mosaic plot}
classification_knn |> get_conf_mat() |> autoplot(type = "mosaic")
```

### Heatmap Matrix
```{r heatmap plot}
classification_knn |> get_conf_mat() |> autoplot(type = "heatmap")
```

Tables and comments
----------------------------------------------------------------------------

### Knn classification
```{r classification knn}
classification_knn |> get_conf_mat()
```

### Precision and recall
```{r compute precision}
classification_knn |> get_pr()
```

### Comments
These matrices show how well the model works. It can be seen that there are 31 *True Positives*, 20 *False Positives*, 18 *False Negatives* and 21 *True Negatives*. The model has an accuracy of 57.8%.  


Decision-tree {data-navmenu="Clustering and Classification"}
========================================================================

```{r random forest basis}
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
indie_forest <- 
  workflow() |> 
  add_recipe(classification_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    compmus_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```

Variable importance 
--------------------------------------------------------------------
### Variable Importance
```{r importance}
workflow() |> 
  add_recipe(classification_recipe) |> 
  add_model(forest_model) |> 
  fit(compmus2025_filtered) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")
```

### Comments
It can be seen that *instrumentalness* is the most influental on the model's prediction whether a track is made by AI or not. Besides this, matrix shows that there are 35 *True Positives*, 16 *False Positives*, 14 *False Negatives* and 25 *True Negatives*. The model has an accuracy of 66.7%.

Further information
------------------------------------------------------------------------------
### Precision and Recall
```{r precision and recall}
indie_forest |> get_pr()
```

### Heatmap Matrix
```{r heatmap plot 2}
indie_forest |> get_conf_mat() |> autoplot(type = "heatmap")
```

Final plot {data-navmenu="Clustering and Classification"}
====================================================================================
Final plot
----------------------------------------------------------------------------------
### Variables which have a strong coherence to AI-prediction
```{r new plot}
compmus2025_filtered |>
  ggplot(aes(x = valence, y = arousal, colour = ai, size = tempo)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Valence",
    y = "Arousal",
    size = "Tempo",
    colour = "AI"
  )
```

> This plot shows the relation of *valence*, *arousal* and *Tempo* to the question whether a track is made by AI or not. 


Track 1 {data-navmenu="Novelty Functions and Tempograms"}
=================================================================================

Novelty functions
---------------------------------------------------------------------------------
### Energy-based novelty function

``` {r energy-based novelty function track 1}
"features/sven-n-1.json" |>
  compmus_energy_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Time (s)", y = "Energy Novelty")
```

### Spectral novelty function

``` {r spectral novelty function track 1}
"features/sven-n-1.json" |>
  compmus_spectral_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Time (s)", y = "Spectral Novelty")
```

### Comments
The *tempogram* of my first track shows very clear lines around 160 BMP, 320 BPM and 480 BPM. The *cyclic tempogram* shows very clear lines around 80 BPM and 160 BPM. Besides this, there is a somewhat vaguer line around 120 BPM. If I tap along with the music, I tap around **160 BPM**, so that would be an accurate estimate. In that case, 330 BPM and 500 BPM would be *tempo upper harmonics* and 80 BPM would be a *tempo subharmonic*. 120 BPM could be seen as a *tempo lower fourth* in relation to 180 BPM.

In the high-level corpus features, the tempo of this track is presented as 112 BPM, which is interestingly enough not that accurate.  

Tempograms
----------------------------------------------------------------------------------
### Tempogram

``` {r tempo gram track 1}
"features/sven-n-1.json" |>
  compmus_tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

### Cyclic Tempogram

``` {r cyclic tempogram track 1}
"features/sven-n-1.json" |>
  compmus_tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```


Track 2 {data-navmenu="Novelty Functions and Tempograms"}
=================================================================================

Novelty functions
---------------------------------------------------------------------------------
### Energy-based novelty function

``` {r energy-based novelty function track 2}
"features/sven-n-2.json" |>
  compmus_energy_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Time (s)", y = "Energy Novelty")
```

### Spectral novelty function

``` {r spectral novelty function track 2}
"features/sven-n-2.json" |>
  compmus_spectral_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Time (s)", y = "Spectral Novelty")
```

### Comments
The *tempogram* of my second track shows very clear lines around 180 BMP, 360 BPM and 540 BPM. The *cyclic tempogram* shows a very clear line around 90 BPM. Besides this, there is a somewhat vaguer line around 135 BPM. If I tap along with the music, I tap around **90 BPM**, so that would be an accurate estimate. In that case, 180 BPM, 360 BPM and 540 BPM would be *tempo upper harmonics*. 135 BPM could be seen as a *tempo fifth* in relation to 90 BPM.

In the high-level corpus features, the tempo of this track is presented as 124 BPM, which is interestingly enough not that accurate. 

Tempograms
----------------------------------------------------------------------------------
### Tempogram

``` {r tempo gram track 2}
"features/sven-n-2.json" |>
  compmus_tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

### Cyclic Tempogram

``` {r cyclic tempogram track 2}
"features/sven-n-2.json" |>
  compmus_tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```


Key- & Chordograms for my own tracks 
=================================================================================

```{r load chord and key information, include=FALSE}
#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

Track 1 grams {.tabset}
-----------------------------------------------------------------------------------------
### Keygram Track 1

``` {r create keygram 1}
"features/sven-n-1.json" |> 
  compmus_chroma(norm = "chebyshev") |> 
  compmus_match_pitch_templates(
    key_templates,  
    norm = "euclidean",  
    distance = "manhattan"
  ) |>
  ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +      
  labs(x = "Time (s)", y = "Template", fill = NULL) +
  theme_classic()
```

### Chordogram Track 1

``` {r create chordogram 1}
"features/sven-n-1.json" |> 
  compmus_chroma(norm = "chebyshev") |> 
  compmus_match_pitch_templates(
    chord_templates,  
    norm = "euclidean",  
    distance = "manhattan"
  ) |>
  ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +      
  labs(x = "Time (s)", y = "Template", fill = NULL) +
  theme_classic()
```

### Comments Track 1

In the prompts I asked the AI music generators to compose a piece in **D major**. Looking at the keygram of track 1, the darkest lines correspond to the keys of *D major*, *D minor* and *A major*. *D minor* is the parallel minor of *D major*. Because of the centricity of the tone *D* within both scales, it is not odd that *D minor* seems prominent in the keygram. The prominence of *A major* can also be explained, because of its neighboring position to *D major* in the circle of fifths. The two keys have six corresponding notes and the tone *A* (as well as the A major chord) has an important function as dominant in the scale of *D major*. 

***

Looking at the chordogram of track 1, the darkest spots appear at the following chords: *D major*, *A major*, *G major*, *B minor* and *F# minor*. These are the tonic, dominant,  subdominant, submediant and mediant of the **D major** scale respectively. The chords are, thus, all diatonic chords and their appearance in the piece are not surprising. Two other remarkable lines are those of *D minor* and *G minor*. These two chords appear in the *D minor* scale, which explains - together with the understanding that this is the parallel minor of *D major* - why they appear in the chordogram as well.


Track 2 grams {.tabset}
------------------------------------------------------------------
### Keygram Track 2

``` {r create keygram 2}
"features/sven-n-2.json" |> 
  compmus_chroma(norm = "chebyshev") |> 
  compmus_match_pitch_templates(
    key_templates,  
    norm = "euclidean",  
    distance = "manhattan" 
  ) |>
  ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +      
  labs(x = "Time (s)", y = "Template", fill = NULL) +
  theme_classic()
```

### Chordogram Track 2

``` {r create chordogram 2}
"features/sven-n-2.json" |> 
  compmus_chroma(norm = "chebyshev") |> 
  compmus_match_pitch_templates(
    chord_templates,  
    norm = "euclidean",  
    distance = "manhattan" 
  ) |>
  ggplot(aes(x = time, y = name, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +      
  labs(x = "Time (s)", y = "Template", fill = NULL) +
  theme_classic()
```

### Comments Track 2

In the prompts I asked the AI music generators to compose a piece in **D major**. Other than track 1, track 2 does not seem to be in *D major*. The *D major* line is light and instead the lines of *C major* and *A major* seem to be the darkest and, thus, the most prominent. However, another line that is remarkable is that of *A minor*, which is not the darkest, but it is on the darker side throughout the whole track. Between 50-100s more importance seems to be given to *C major* and between 110-160s this seems to shift to *A major*. *A minor* can in this sense be seen as the bridge between both scales: *C major* is its relative major key and *A major* is its parallel major key. Although the track is not in *D major*, the other three scales seem to work together (through modulation) to create the tonal space in this track. 

***

Looking at the chordogram of track 2, the chord progression seems to show modulations as well. 0-50s puts the most emphasis on the *A minor* chord. *A major* and *A7* is also apparent, likely because of the centricity of the tone *A*. 50-110s shows a shift to the *C major*, *C minor*, *F minor* chords (perhaps then not a shift to the key of *C major*, but to the key of *C minor*?), in which the *A major* line suddenly gets very bright compared to the former section (the chord is unlikely to appear in this section). 110-130s shows a shift again, emphasizing the chords of *A major* , *A minor*, *F minor* and *D minor* - a seemingly combination of the *A major* and *A minor* key. After 130s, the *F minor* chord dissapears again, which puts a bigger emphasis on the *A minor* and *D minor* chords. With these chords, the track seems to end in the **A minor** key.


Track 1 {data-navmenu="SSMs for my own tracks"} 
==================================================================================

Chromagram and Cepstrogram
-----------------------------------------------------------------------------------------
### Chromagram

``` {r create chromagram 2}
"features/sven-n-2.json" |>            
  compmus_chroma(norm = "identity") |>  
  ggplot(aes(x = time, y = pc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:11,
    minor_breaks = NULL,
    labels = c(
                "C", "C#|Db", "D", "D#|Eb",
                "E", "F", "F#|Gb", "G",
                "G#|Ab", "A", "A#|Bb", "B"
              )
  ) +
  scale_fill_viridis_c(guide = "none") +  
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()                           
```

### Cepstrogram

``` {r create cepstrogram 2}
"features/sven-n-2.json" |>              
  compmus_mfccs(norm = "identity") |>       
  ggplot(aes(x = time, y = mfcc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:12,
    minor_breaks = NULL,
  ) +
  scale_fill_viridis_c(guide = "none") +        
  labs(x = "Time (s)", y = "Coefficient Number", fill = NULL) +
  theme_classic()   
```

### Comments
Track two seems somewhat structured in the *chroma-based matrics*, but also in the *timbre-based matrice*. In the *chroma-based matrice*, I think there is an A1-B-A2-?A3? structure. However the structure in the *timbre-based matrice* is harder to point out, because it seems to change a lot.


Self-similarity Matrices
-------------------------------------------------------------------------------
### Chroma-based Matrice

```{r create chroma ssm 2}
"features/sven-n-2.json" |>          
  compmus_chroma(norm = "chebyshev") |>        
  compmus_self_similarity(
    feature = pc,
    distance = "manhattan"                  
  ) |>   
  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +      
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()      
```

### Timbre-based Matrice

```{r create timbre ssm 2}
"features/sven-n-2.json" |>                  
  compmus_mfccs(norm = "euclidean") |>       
  compmus_self_similarity(
    feature = mfcc,
    distance = "euclidean"              
  ) |>   
  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +        
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic() 
```


Track 2 {data-navmenu="SSMs for my own tracks"} 
===================================================================================

Chromagram and Cepstrogram
-----------------------------------------------------------------------------------------
### Chromagram

``` {r create chromagram 1}
"features/sven-n-1.json" |>            
  compmus_chroma(norm = "identity") |>  
  ggplot(aes(x = time, y = pc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:11,
    minor_breaks = NULL,
    labels = c(
                "C", "C#|Db", "D", "D#|Eb",
                "E", "F", "F#|Gb", "G",
                "G#|Ab", "A", "A#|Bb", "B"
              )
  ) +
  scale_fill_viridis_c(guide = "none") +  
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()                           
```

### Cepstrogram

``` {r create cepstrogram 1}
"features/sven-n-1.json" |>              
  compmus_mfccs(norm = "identity") |>       
  ggplot(aes(x = time, y = mfcc, fill = value)) + 
  geom_raster() +
  scale_y_continuous(
    breaks = 0:12,
    minor_breaks = NULL,
  ) +
  scale_fill_viridis_c(guide = "none") +        
  labs(x = "Time (s)", y = "Coefficient Number", fill = NULL) +
  theme_classic()   
```

### Comments
Track one seems not-so structured in the *chroma-based matrics*, but a little more in the *timbre-based matrice*. In the *chroma-based matrice*, I think there is hardly and structure present, it seems to change the whole time. However the structure in the *timbre-based matrice* seems to be some A-B-C structure, in which the timbre changes a bit between each section. The biggest change, however, occurs between section B and C.


Self-similarity Matrices
---------------------------------------------------------------------------------
### Chroma-based Matrice

```{r create chroma ssm 1}
"features/sven-n-1.json" |>          
  compmus_chroma(norm = "chebyshev") |>        
  compmus_self_similarity(
    feature = pc,
    distance = "manhattan"                  
  ) |>   
  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +      
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic()      
```

### Timbre-based Matrice

```{r create timbre ssm 1}
"features/sven-n-1.json" |>                  
  compmus_mfccs(norm = "euclidean") |>       
  compmus_self_similarity(
    feature = mfcc,
    distance = "euclidean"              
  ) |>   
  ggplot(aes(x = xtime, y = ytime, fill = d)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +        
  labs(x = "Time (s)", y = NULL, fill = NULL) +
  theme_classic() 
```


Tables class corpus
==================================================================================

Column 1 
----------------------------------------------------------------------------------
### Compmus2025 Table

```{r friendly table}
datatable(compmus2025, rownames = FALSE, options = list(pageLength = 13))
```

> This table shows a diverse range of information about the tracks. Besides the *filenames*, it displayes the *approachability*, *arousal*, *danceability*, *engagingness*, *instrumentalness*, *tempo*, *valence* and the *names* of the students.The information in this table will be the base for my first visualisation.


Class corpus visualisations {.storyboard}
=====================================================================================
### Comparison of all songs

```{r}
Comparison_gg <- ggplot(compmus2025, aes(x = arousal, y = danceability, colour = valence)) + geom_jitter(alpha = 0.6) 

ggplotly(Comparison_gg)
```

***
This graph shows the relationship between the *danceability* and *arousal*. Especially at the beginning of the graph, it seems like there is a correlation: when *danceability* rises, so does *arousal*. Combining this with *engagingness*, which is shown by the size of the dots, it can be seen that when *danceability* and *arousal* rise, there is often also an increase in *engagingness*. 

***
Lastly *valence* is added to the graph as the colour of the dots. It can be seen that often *valence* and *engagingness* increase together. However, at certain moments, the relationship between *valence* and *engagingness* is less apparent. 

### Whose song is the most danceable?

```{r}
Dance_gg <- ggplot(compmus2025, aes(x = reorder(name, danceability), y = danceability, fill = ai)) + geom_col(alpha = 0.9) + theme(axis.text.x = element_text(angle = 45, size = 7))

ggplotly(Dance_gg)
```

***
In this graph, the mean of *danceability* of the two tracks of each person is shown. It can be seen that the rate of *danceability* is very diverse: Roemer made the most danceable tracks with a mean of 0.9997242 as score and Erik made the least danceable tracks with 0.1242007 as score. 


Conclusion and Discussion
==================================================================
### Own tracks in compmus2025

**AI Prompts:**
For the first track, I used https://www.jenmusic.ai to create it and provided it with the following prompt: “I would like a 6/8 meter, often stressing this meter, but at certain moments provided with hemiolas. It should be a ballad, structured as: intro - verse - pre-chorus - chorus - break - verse - chorus - chorus - outro. As instruments, it should use a piano, soft percussion, a string quartet and some woodwinds. The piano provides a progression consisting of chords with additional notes (such as 7ths, 9ths, 11ths, 13ths) and sus-chords. The soft percussion starts minimal, but gets more extensive in the choruses. The strings play throughout the whole song, but the viola mainly provides the melody. The woodwinds are only used to emphasize the pre-chorus and the chorus. The ballad should be in D major as key and has a slow to medium tempo. The reverb should be as if the piece is played in a small hall.” I tried to be specific in many aspects to see if the AI tool would create something that would meet my ‘demands’. Even though it did not in many ways, it picked up on some features of the prompt.

For the second track, I used https://www.stableaudio.com to create it and provided it with the following prompt: “soft ballad, 6/8, D major, slow to medium tempo, piano, soft percussion gets more extensive over-time, string quartet, melody played by viola, soft woodwinds, triad chords played by piano, added note chords played by piano, hemiolas, small hall reverb.” This tool specified that it would work better when using short descriptions, so I tried to recreate the prompt I used for track one as much as I could with this in mind. Some ‘demands’ are now less elaborate or not mentioned at all. Again, it does seem to leave out some ‘demands’, but many others are to some extent met.

***
**Own tracks:**
My own tracks are low in *danceability* and *arousal*, but in the middle on the scale of *engagingness* and *valence*. Compared to all the other tracks, my tracks are in the lower spectrum of each category. I find it interesting how Essentia labelled the *tempo* of my tracks as 112 and 124 respectively, when I asked the AI models to generate slow (to medium) tempo music. 









